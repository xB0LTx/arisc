{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Laboratorium 3\n",
    "## Marcin Ogórkiewicz\n",
    "### 21.05.2025"
   ],
   "id": "c6a1607ef8c3f826"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zadanie 1\n",
    "#### Wybrać 3 środowiska dostępne w samej bibliotece Stable Baselines 3, lub comunity wykorzystujących jej API. Zaleca się, aby były to jak najbardziej różne środowiska"
   ],
   "id": "8ceba0d93e431771"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:27:56.154613Z",
     "start_time": "2025-05-21T13:27:56.148514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Sprawdzanie dostępności CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "print(f\"CUDA dostępna: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Środowisko 1:\n",
    "env1_name = \"CartPole-v1\"\n",
    "env1 = gym.make(env1_name)\n",
    "print(f\"\\nŚrodowisko 1: {env1_name}\")\n",
    "print(f\"Przestrzeń obserwacji: {env1.observation_space}\")\n",
    "print(f\"Przestrzeń akcji: {env1.action_space}\")\n",
    "\n",
    "# Środowisko 2:\n",
    "env2_name = \"MountainCar-v0\"\n",
    "env2 = gym.make(env2_name)\n",
    "print(f\"\\nŚrodowisko 2: {env2_name}\")\n",
    "print(f\"Przestrzeń obserwacji: {env2.observation_space}\")\n",
    "print(f\"Przestrzeń akcji: {env2.action_space}\")\n",
    "\n",
    "# Środowisko 3:\n",
    "env3_name = \"Acrobot-v1\"\n",
    "env3 = gym.make(env3_name)\n",
    "print(f\"\\nŚrodowisko 3: {env3_name}\")\n",
    "print(f\"Przestrzeń obserwacji: {env3.observation_space}\")\n",
    "print(f\"Przestrzeń akcji: {env3.action_space}\")"
   ],
   "id": "c854cf9e9b9beed3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Używane urządzenie: cuda\n",
      "CUDA dostępna: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "\n",
      "Środowisko 1: CartPole-v1\n",
      "Przestrzeń obserwacji: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Przestrzeń akcji: Discrete(2)\n",
      "\n",
      "Środowisko 2: MountainCar-v0\n",
      "Przestrzeń obserwacji: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Przestrzeń akcji: Discrete(3)\n",
      "\n",
      "Środowisko 3: Acrobot-v1\n",
      "Przestrzeń obserwacji: Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)\n",
      "Przestrzeń akcji: Discrete(3)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zadanie 2\n",
    "#### Dla każdego z wybranych środowisk dobrać model z biblioteki Stable Baselines 3"
   ],
   "id": "8a17297ddf09348d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:27:56.223022Z",
     "start_time": "2025-05-21T13:27:56.203761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "\n",
    "# Dla CartPole (dyskretna przestrzeń akcji, prosta dynamika)\n",
    "model1 = PPO(\"MlpPolicy\", env1, verbose=1, device=device)\n",
    "\n",
    "# Dla MountainCar (dyskretna przestrzeń akcji, bardziej złożona dynamika)\n",
    "model2 = DQN(\"MlpPolicy\", env2, verbose=1, device=device,\n",
    "             learning_rate=1e-3,\n",
    "             buffer_size=50000,\n",
    "             learning_starts=1000,\n",
    "             batch_size=64,\n",
    "             exploration_fraction=0.2)\n",
    "\n",
    "# Dla Acrobot (dyskretna przestrzeń akcji, złożona dynamika)\n",
    "model3 = A2C(\"MlpPolicy\", env3, verbose=1, device=device,\n",
    "            learning_rate=7e-4)\n",
    "\n",
    "print(\"\\nWybrane modele z akceleracją CUDA:\")\n",
    "print(f\"Środowisko 1 ({env1_name}): PPO\")\n",
    "print(f\"Środowisko 2 ({env2_name}): DQN\")\n",
    "print(f\"Środowisko 3 ({env3_name}): A2C\")\n",
    "\n",
    "# Wyjaśnienie wyboru modeli\n",
    "print(\"\\nUzasadnienie wyboru modeli:\")\n",
    "print(\"- PPO dla CartPole: Dobry dla prostych środowisk z dyskretną przestrzenią akcji, stabilne uczenie\")\n",
    "print(\"- DQN dla MountainCar: Skuteczny dla dyskretnych przestrzeni akcji z opóźnioną nagrodą\")\n",
    "print(\"- A2C dla Acrobot: Efektywny algorytm policy-gradient dla środowisk z dyskretną przestrzenią akcji\")"
   ],
   "id": "55a59cb1238d4e66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "Wybrane modele z akceleracją CUDA:\n",
      "Środowisko 1 (CartPole-v1): PPO\n",
      "Środowisko 2 (MountainCar-v0): DQN\n",
      "Środowisko 3 (Acrobot-v1): A2C\n",
      "\n",
      "Uzasadnienie wyboru modeli:\n",
      "- PPO dla CartPole: Dobry dla prostych środowisk z dyskretną przestrzenią akcji, stabilne uczenie\n",
      "- DQN dla MountainCar: Skuteczny dla dyskretnych przestrzeni akcji z opóźnioną nagrodą\n",
      "- A2C dla Acrobot: Efektywny algorytm policy-gradient dla środowisk z dyskretną przestrzenią akcji\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zadanie 3\n",
    "#### Dla każdego z wybranych środowisk przeprowadzić trening modelu"
   ],
   "id": "712b8b23f206c72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:36:32.647337Z",
     "start_time": "2025-05-21T13:27:56.268580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Tworzenie katalogów do zapisu modeli\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Tworzenie środowisk ewaluacyjnych\n",
    "eval_env1 = Monitor(gym.make(env1_name))\n",
    "eval_env2 = Monitor(gym.make(env2_name))\n",
    "eval_env3 = Monitor(gym.make(env3_name))\n",
    "\n",
    "# Tworzenie callbacków do ewaluacji\n",
    "eval_callback1 = EvalCallback(eval_env1, best_model_save_path=\"./trained_models/model1/\",\n",
    "                             log_path=\"./logs/\", eval_freq=1000,\n",
    "                             deterministic=True, render=False, n_eval_episodes=5)\n",
    "\n",
    "eval_callback2 = EvalCallback(eval_env2, best_model_save_path=\"./trained_models/model2/\",\n",
    "                             log_path=\"./logs/\", eval_freq=1000,\n",
    "                             deterministic=True, render=False, n_eval_episodes=5)\n",
    "\n",
    "eval_callback3 = EvalCallback(eval_env3, best_model_save_path=\"./trained_models/model3/\",\n",
    "                             log_path=\"./logs/\", eval_freq=1000,\n",
    "                             deterministic=True, render=False, n_eval_episodes=5)\n",
    "\n",
    "# Parametry treningu - zoptymalizowane dla akceleracji GPU\n",
    "timesteps1 = 50000   # CartPole\n",
    "timesteps2 = 100000  # MountainCar\n",
    "timesteps3 = 70000   # Acrobot\n",
    "\n",
    "training_results = []\n",
    "\n",
    "# Trening modelu 1\n",
    "print(f\"\\nTrening modelu dla {env1_name} przy użyciu {device}...\")\n",
    "start_time = time.time()\n",
    "model1.learn(total_timesteps=timesteps1, callback=eval_callback1)\n",
    "end_time = time.time()\n",
    "training_time1 = end_time - start_time\n",
    "model1.save(f\"trained_models/final_ppo_{env1_name}\")\n",
    "training_results.append({\n",
    "    \"Środowisko\": env1_name,\n",
    "    \"Algorytm\": \"PPO\",\n",
    "    \"Czas treningu (s)\": training_time1,\n",
    "    \"Całkowita liczba kroków\": timesteps1\n",
    "})\n",
    "print(f\"Trening zakończony w {training_time1:.2f} sekund\")\n",
    "\n",
    "# Trening modelu 2\n",
    "print(f\"\\nTrening modelu dla {env2_name} przy użyciu {device}...\")\n",
    "start_time = time.time()\n",
    "model2.learn(total_timesteps=timesteps2, callback=eval_callback2)\n",
    "end_time = time.time()\n",
    "training_time2 = end_time - start_time\n",
    "model2.save(f\"trained_models/final_dqn_{env2_name}\")\n",
    "training_results.append({\n",
    "    \"Środowisko\": env2_name,\n",
    "    \"Algorytm\": \"DQN\",\n",
    "    \"Czas treningu (s)\": training_time2,\n",
    "    \"Całkowita liczba kroków\": timesteps2\n",
    "})\n",
    "print(f\"Trening zakończony w {training_time2:.2f} sekund\")\n",
    "\n",
    "# Trening modelu 3\n",
    "print(f\"\\nTrening modelu dla {env3_name} przy użyciu {device}...\")\n",
    "start_time = time.time()\n",
    "model3.learn(total_timesteps=timesteps3, callback=eval_callback3)\n",
    "end_time = time.time()\n",
    "training_time3 = end_time - start_time\n",
    "model3.save(f\"trained_models/final_a2c_{env3_name}\")\n",
    "training_results.append({\n",
    "    \"Środowisko\": env3_name,\n",
    "    \"Algorytm\": \"A2C\",\n",
    "    \"Czas treningu (s)\": training_time3,\n",
    "    \"Całkowita liczba kroków\": timesteps3\n",
    "})\n",
    "print(f\"Trening zakończony w {training_time3:.2f} sekund\")\n",
    "\n",
    "print(\"\\nWszystkie modele zostały wytrenowane i zapisane.\")\n",
    "\n",
    "# Wyświetlanie informacji o czasie treningu\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "df = pd.DataFrame(training_results)\n",
    "print(\"\\nPodsumowanie wydajności treningu:\")\n",
    "print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))"
   ],
   "id": "f56e78f77fb0b005",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trening modelu dla CartPole-v1 przy użyciu cuda...\n",
      "Eval num_timesteps=1000, episode_reward=57.00 +/- 27.22\n",
      "Episode length: 57.00 +/- 27.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 57       |\n",
      "|    mean_reward     | 57       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=56.60 +/- 20.92\n",
      "Episode length: 56.60 +/- 20.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 56.6     |\n",
      "|    mean_reward     | 56.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 729      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=172.60 +/- 29.98\n",
      "Episode length: 172.60 +/- 29.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 173         |\n",
      "|    mean_reward          | 173         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008278295 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.0006     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.95        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 56.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=152.80 +/- 16.71\n",
      "Episode length: 152.80 +/- 16.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 153      |\n",
      "|    mean_reward     | 153      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.5     |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=332.80 +/- 188.27\n",
      "Episode length: 332.80 +/- 188.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 333         |\n",
      "|    mean_reward          | 333         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009804294 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.0941      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 34          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=184.20 +/- 142.30\n",
      "Episode length: 184.20 +/- 142.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 184      |\n",
      "|    mean_reward     | 184      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.4     |\n",
      "|    ep_rew_mean     | 31.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 446      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=209.60 +/- 149.63\n",
      "Episode length: 209.60 +/- 149.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 210         |\n",
      "|    mean_reward          | 210         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009574439 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 46.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=164.40 +/- 57.62\n",
      "Episode length: 164.40 +/- 57.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 164      |\n",
      "|    mean_reward     | 164      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43.2     |\n",
      "|    ep_rew_mean     | 43.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 434      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=248.80 +/- 68.49\n",
      "Episode length: 248.80 +/- 68.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 249         |\n",
      "|    mean_reward          | 249         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009222688 |\n",
      "|    clip_fraction        | 0.078       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 59.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=327.60 +/- 124.25\n",
      "Episode length: 327.60 +/- 124.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 328      |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58       |\n",
      "|    ep_rew_mean     | 58       |\n",
      "| time/              |          |\n",
      "|    fps             | 428      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=399.60 +/- 98.23\n",
      "Episode length: 399.60 +/- 98.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 400         |\n",
      "|    mean_reward          | 400         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012069717 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.2        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 63          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=434.20 +/- 101.86\n",
      "Episode length: 434.20 +/- 101.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 434      |\n",
      "|    mean_reward     | 434      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.5     |\n",
      "|    ep_rew_mean     | 73.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 408      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=408.40 +/- 76.54\n",
      "Episode length: 408.40 +/- 76.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 408         |\n",
      "|    mean_reward          | 408         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006841504 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=460.20 +/- 79.60\n",
      "Episode length: 460.20 +/- 79.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 460      |\n",
      "|    mean_reward     | 460      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.4     |\n",
      "|    ep_rew_mean     | 89.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 391      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010202801 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.39        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00993    |\n",
      "|    value_loss           | 42.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=454.60 +/- 56.69\n",
      "Episode length: 454.60 +/- 56.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | 455      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 375      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=449.40 +/- 62.04\n",
      "Episode length: 449.40 +/- 62.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 449          |\n",
      "|    mean_reward          | 449          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059536416 |\n",
      "|    clip_fraction        | 0.0542       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.792        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00691     |\n",
      "|    value_loss           | 34.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=458.60 +/- 40.27\n",
      "Episode length: 458.60 +/- 40.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 459      |\n",
      "|    mean_reward     | 459      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=389.00 +/- 68.93\n",
      "Episode length: 389.00 +/- 68.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 389          |\n",
      "|    mean_reward          | 389          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073712333 |\n",
      "|    clip_fraction        | 0.0695       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.569       |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.17         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00639     |\n",
      "|    value_loss           | 36.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=396.20 +/- 85.10\n",
      "Episode length: 396.20 +/- 85.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 396      |\n",
      "|    mean_reward     | 396      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=313.80 +/- 15.22\n",
      "Episode length: 313.80 +/- 15.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 314          |\n",
      "|    mean_reward          | 314          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 21000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059627425 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29           |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    value_loss           | 43.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=289.60 +/- 17.17\n",
      "Episode length: 289.60 +/- 17.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 290      |\n",
      "|    mean_reward     | 290      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 161      |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=462.80 +/- 49.11\n",
      "Episode length: 462.80 +/- 49.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 463         |\n",
      "|    mean_reward          | 463         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012545506 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.2         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00838    |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=462.00 +/- 53.59\n",
      "Episode length: 462.00 +/- 53.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 176      |\n",
      "|    ep_rew_mean     | 176      |\n",
      "| time/              |          |\n",
      "|    fps             | 360      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008330176 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.6         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 197      |\n",
      "|    ep_rew_mean     | 197      |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006724946 |\n",
      "|    clip_fraction        | 0.0629      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.0442      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.94        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    value_loss           | 25.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 213      |\n",
      "|    ep_rew_mean     | 213      |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007146366 |\n",
      "|    clip_fraction        | 0.0505      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00223    |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 232      |\n",
      "|    ep_rew_mean     | 232      |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080367215 |\n",
      "|    clip_fraction        | 0.0932       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.518       |\n",
      "|    explained_variance   | 0.207        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.12         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00664     |\n",
      "|    value_loss           | 1.31         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 249      |\n",
      "|    ep_rew_mean     | 249      |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040643485 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.00803      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0735       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000514    |\n",
      "|    value_loss           | 0.851        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 266      |\n",
      "|    ep_rew_mean     | 266      |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059880526 |\n",
      "|    clip_fraction        | 0.0655       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | 0.0481       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0211       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00438     |\n",
      "|    value_loss           | 0.519        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 285      |\n",
      "|    ep_rew_mean     | 285      |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 110      |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021760925 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | 0.171        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0146       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    value_loss           | 0.334        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | 304      |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004455247 |\n",
      "|    clip_fraction        | 0.0166      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | -0.00587    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00318     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 321      |\n",
      "|    ep_rew_mean     | 321      |\n",
      "| time/              |          |\n",
      "|    fps             | 333      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024112505 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.0278       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0175       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000577    |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 338      |\n",
      "|    ep_rew_mean     | 338      |\n",
      "| time/              |          |\n",
      "|    fps             | 329      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051047252 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.528       |\n",
      "|    explained_variance   | -0.044       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.019        |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 0.0811       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 352      |\n",
      "|    ep_rew_mean     | 352      |\n",
      "| time/              |          |\n",
      "|    fps             | 329      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004828763 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | -4.99e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    value_loss           | 0.052       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 367      |\n",
      "|    ep_rew_mean     | 367      |\n",
      "| time/              |          |\n",
      "|    fps             | 329      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043030893 |\n",
      "|    clip_fraction        | 0.0176       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | -0.0177      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0152       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 0.0312       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 381      |\n",
      "|    ep_rew_mean     | 381      |\n",
      "| time/              |          |\n",
      "|    fps             | 329      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00469806 |\n",
      "|    clip_fraction        | 0.0543     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.517     |\n",
      "|    explained_variance   | 0.165      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00275   |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.00568   |\n",
      "|    value_loss           | 0.0204     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 395      |\n",
      "|    ep_rew_mean     | 395      |\n",
      "| time/              |          |\n",
      "|    fps             | 329      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Trening zakończony w 156.31 sekund\n",
      "\n",
      "Trening modelu dla MountainCar-v0 przy użyciu cuda...\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 12978    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 800      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1931     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.96e-05 |\n",
      "|    n_updates        | 149      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.44e-05 |\n",
      "|    n_updates        | 249      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.886    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1406     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.63e-05 |\n",
      "|    n_updates        | 349      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.858    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 3000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.03e-06 |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1240     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 3200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.17e-07 |\n",
      "|    n_updates        | 549      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.15e-07 |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1152     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 4000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1201     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 4800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.42e-07 |\n",
      "|    n_updates        | 949      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.12e-07 |\n",
      "|    n_updates        | 999      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.734    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1135     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 5600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.49e-07 |\n",
      "|    n_updates        | 1149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.02e-08 |\n",
      "|    n_updates        | 1249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1076     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 6400     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.7e-07  |\n",
      "|    n_updates        | 1349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.668    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 7000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.65e-08 |\n",
      "|    n_updates        | 1499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1040     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 7200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.22e-07 |\n",
      "|    n_updates        | 1549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.05e-08 |\n",
      "|    n_updates        | 1749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1013     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.582    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1039     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 8800     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.61e-08 |\n",
      "|    n_updates        | 1949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.573    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 9000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.56e-08 |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1015     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 9600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.8e-08  |\n",
      "|    n_updates        | 2149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.59e-08 |\n",
      "|    n_updates        | 2249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.506    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 996      |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 10400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.19e-05 |\n",
      "|    n_updates        | 2349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 11000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.46e-06 |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.468    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 978      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 11200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.4e-06  |\n",
      "|    n_updates        | 2549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.38e-06 |\n",
      "|    n_updates        | 2749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 964      |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 12000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.392    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 979      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 12800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.59e-07 |\n",
      "|    n_updates        | 2949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 13000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.6e-07  |\n",
      "|    n_updates        | 2999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.354    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 963      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 13600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.14e-07 |\n",
      "|    n_updates        | 3149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 14000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.69e-07 |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.316    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 14400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.16e-07 |\n",
      "|    n_updates        | 3349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.288    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.32e-06 |\n",
      "|    n_updates        | 3499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.278    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 937      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 15200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.39e-07 |\n",
      "|    n_updates        | 3549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.54e-07 |\n",
      "|    n_updates        | 3749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 926      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 16000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.202    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 935      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 16800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.92e-07 |\n",
      "|    n_updates        | 3949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.193    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.04e-07 |\n",
      "|    n_updates        | 3999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.164    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 925      |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 17600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.43e-07 |\n",
      "|    n_updates        | 4149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.6e-07  |\n",
      "|    n_updates        | 4249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.126    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 914      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 18400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.31e-07 |\n",
      "|    n_updates        | 4349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0975   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 19000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.62e-07 |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.088    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 905      |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 19200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.38e-07 |\n",
      "|    n_updates        | 4549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.54e-07 |\n",
      "|    n_updates        | 4749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 896      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 903      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 20800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.24e-05 |\n",
      "|    n_updates        | 4949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 21000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.93e-06 |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 895      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 21600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.78e-07 |\n",
      "|    n_updates        | 5149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 22000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.17e-07 |\n",
      "|    n_updates        | 5249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 888      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 22400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.71e-07 |\n",
      "|    n_updates        | 5349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 23000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.59e-07 |\n",
      "|    n_updates        | 5499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 873      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 23200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.58e-07 |\n",
      "|    n_updates        | 5549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 24000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.19e-08 |\n",
      "|    n_updates        | 5749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 862      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 24000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 867      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 24800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.24e-08 |\n",
      "|    n_updates        | 5949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.13e-08 |\n",
      "|    n_updates        | 5999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 859      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 25600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.91e-08 |\n",
      "|    n_updates        | 6149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 26000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.91e-08 |\n",
      "|    n_updates        | 6249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 853      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 26400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.96e-08 |\n",
      "|    n_updates        | 6349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 27000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.65e-08 |\n",
      "|    n_updates        | 6499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 847      |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 27200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.8e-08  |\n",
      "|    n_updates        | 6549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.74e-08 |\n",
      "|    n_updates        | 6749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 842      |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 28000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 847      |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 28800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.73e-08 |\n",
      "|    n_updates        | 6949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 29000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.61e-08 |\n",
      "|    n_updates        | 6999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 842      |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 29600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.47e-08 |\n",
      "|    n_updates        | 7149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.35e-08 |\n",
      "|    n_updates        | 7249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 837      |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 30400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.16e-05 |\n",
      "|    n_updates        | 7349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 31000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.24e-07 |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 833      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 31200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.04e-06 |\n",
      "|    n_updates        | 7549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 32000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.4e-07  |\n",
      "|    n_updates        | 7749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 829      |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 32000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 834      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 32800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.79e-07 |\n",
      "|    n_updates        | 7949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 33000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.53e-07 |\n",
      "|    n_updates        | 7999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 830      |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 33600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.02e-07 |\n",
      "|    n_updates        | 8149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 34000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.74e-07 |\n",
      "|    n_updates        | 8249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 826      |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 34400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.67e-07 |\n",
      "|    n_updates        | 8349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.38e-07 |\n",
      "|    n_updates        | 8499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 822      |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 35200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.57e-07 |\n",
      "|    n_updates        | 8549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 36000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.04e-08 |\n",
      "|    n_updates        | 8749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 819      |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 36000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 823      |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 36800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.14e-07 |\n",
      "|    n_updates        | 8949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 37000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.75e-07 |\n",
      "|    n_updates        | 8999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 820      |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 37600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.15e-08 |\n",
      "|    n_updates        | 9149     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.24e-07 |\n",
      "|    n_updates        | 9249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 816      |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 38400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.18e-08 |\n",
      "|    n_updates        | 9349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 39000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.05e-07 |\n",
      "|    n_updates        | 9499     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 813      |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 39200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.23e-07 |\n",
      "|    n_updates        | 9549     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.64e-08 |\n",
      "|    n_updates        | 9749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 811      |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 814      |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 40800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.67e-07 |\n",
      "|    n_updates        | 9949     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 41000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.06e-07 |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 811      |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 41600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.95e-08 |\n",
      "|    n_updates        | 10149    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 42000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.6e-07  |\n",
      "|    n_updates        | 10249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 808      |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 42400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.51e-08 |\n",
      "|    n_updates        | 10349    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 43000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.41e-08 |\n",
      "|    n_updates        | 10499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 806      |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 43200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.16e-07 |\n",
      "|    n_updates        | 10549    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 44000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.57e-08 |\n",
      "|    n_updates        | 10749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 804      |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 44000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 807      |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 44800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.3e-08  |\n",
      "|    n_updates        | 10949    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.3e-07  |\n",
      "|    n_updates        | 10999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 805      |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 45600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.59e-08 |\n",
      "|    n_updates        | 11149    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 46000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.37e-07 |\n",
      "|    n_updates        | 11249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 803      |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 46400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.73e-08 |\n",
      "|    n_updates        | 11349    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 47000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.92e-07 |\n",
      "|    n_updates        | 11499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 801      |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 47200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.88e-07 |\n",
      "|    n_updates        | 11549    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 48000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.25e-08 |\n",
      "|    n_updates        | 11749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 799      |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 48000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 802      |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 48800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.74e-08 |\n",
      "|    n_updates        | 11949    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 49000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.8e-08  |\n",
      "|    n_updates        | 11999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 800      |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 49600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.6e-08  |\n",
      "|    n_updates        | 12149    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.63e-08 |\n",
      "|    n_updates        | 12249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 798      |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 50400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.09e-05 |\n",
      "|    n_updates        | 12349    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 51000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.68e-06 |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 797      |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 51200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.5e-06  |\n",
      "|    n_updates        | 12549    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.68e-07 |\n",
      "|    n_updates        | 12749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 52000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 798      |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 52800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.76e-07 |\n",
      "|    n_updates        | 12949    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 53000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.48e-07 |\n",
      "|    n_updates        | 12999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 796      |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 53600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.19e-07 |\n",
      "|    n_updates        | 13149    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.71e-07 |\n",
      "|    n_updates        | 13249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 794      |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 54400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.73e-07 |\n",
      "|    n_updates        | 13349    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.88e-08 |\n",
      "|    n_updates        | 13499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 793      |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 55200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.53e-06 |\n",
      "|    n_updates        | 13549    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.98e-08 |\n",
      "|    n_updates        | 13749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 56000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 794      |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 56800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.91e-08 |\n",
      "|    n_updates        | 13949    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 57000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.39e-08 |\n",
      "|    n_updates        | 13999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 793      |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 57600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.88e-08 |\n",
      "|    n_updates        | 14149    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 58000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.01e-08 |\n",
      "|    n_updates        | 14249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 792      |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 58400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.97e-08 |\n",
      "|    n_updates        | 14349    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.47e-08 |\n",
      "|    n_updates        | 14499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 790      |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 59200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.8e-08  |\n",
      "|    n_updates        | 14549    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.13e-07 |\n",
      "|    n_updates        | 14749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 60000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 792      |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 60789    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.92e-06 |\n",
      "|    n_updates        | 14947    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.38e-07 |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 790      |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 61589    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.36e-07 |\n",
      "|    n_updates        | 15147    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.11e-07 |\n",
      "|    n_updates        | 15249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 62389    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.84e-07 |\n",
      "|    n_updates        | 15347    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.31e-07 |\n",
      "|    n_updates        | 15499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 788      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 63189    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.27e-06 |\n",
      "|    n_updates        | 15547    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 63989    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.93e-07 |\n",
      "|    n_updates        | 15747    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.52e-06 |\n",
      "|    n_updates        | 15749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 64789    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.26e-07 |\n",
      "|    n_updates        | 15947    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.79e-07 |\n",
      "|    n_updates        | 15999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 788      |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 65589    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.58e-07 |\n",
      "|    n_updates        | 16147    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.33e-07 |\n",
      "|    n_updates        | 16249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 787      |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 66389    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.46e-07 |\n",
      "|    n_updates        | 16347    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.68e-06 |\n",
      "|    n_updates        | 16499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 786      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 67189    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.49e-07 |\n",
      "|    n_updates        | 16547    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 788      |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 67989    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.67e-07 |\n",
      "|    n_updates        | 16747    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.87e-07 |\n",
      "|    n_updates        | 16749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 787      |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 68789    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.95e-07 |\n",
      "|    n_updates        | 16947    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.21e-07 |\n",
      "|    n_updates        | 16999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 199      |\n",
      "|    ep_rew_mean      | -199     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 785      |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 69510    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.22e-06 |\n",
      "|    n_updates        | 17127    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.71e-06 |\n",
      "|    n_updates        | 17249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 198      |\n",
      "|    ep_rew_mean      | -198     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 784      |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 70250    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000474 |\n",
      "|    n_updates        | 17312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.28e-06 |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 198      |\n",
      "|    ep_rew_mean      | -198     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 783      |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 71029    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.58e-06 |\n",
      "|    n_updates        | 17507    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 197      |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 784      |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 71686    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.29e-06 |\n",
      "|    n_updates        | 17671    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.22e-06 |\n",
      "|    n_updates        | 17749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 196      |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 782      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 72417    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000213 |\n",
      "|    n_updates        | 17854    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.42e-06 |\n",
      "|    n_updates        | 17999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 196      |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 781      |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 73217    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.83e-06 |\n",
      "|    n_updates        | 18054    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 195      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 783      |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 73939    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.16e-06 |\n",
      "|    n_updates        | 18234    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.49e-06 |\n",
      "|    n_updates        | 18249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 195      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 782      |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 74739    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.08e-06 |\n",
      "|    n_updates        | 18434    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.59e-06 |\n",
      "|    n_updates        | 18499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 195      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 781      |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 75539    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.89e-06 |\n",
      "|    n_updates        | 18634    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.37e-05 |\n",
      "|    n_updates        | 18749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 195      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 780      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 76332    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.96e-07 |\n",
      "|    n_updates        | 18832    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.94e-05 |\n",
      "|    n_updates        | 18999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 194      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 778      |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 77049    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.06e-06 |\n",
      "|    n_updates        | 19012    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 194      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 781      |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 77849    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.13e-05 |\n",
      "|    n_updates        | 19212    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.6e-06  |\n",
      "|    n_updates        | 19249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 194      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 779      |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 78598    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.3e-05  |\n",
      "|    n_updates        | 19399    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.62e-06 |\n",
      "|    n_updates        | 19499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 194      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 778      |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 79398    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.96e-05 |\n",
      "|    n_updates        | 19599    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.22e-06 |\n",
      "|    n_updates        | 19749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 194      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 777      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 80198    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000949 |\n",
      "|    n_updates        | 19799    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 193      |\n",
      "|    ep_rew_mean      | -193     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 779      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 80853    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000135 |\n",
      "|    n_updates        | 19963    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.14e-05 |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 192      |\n",
      "|    ep_rew_mean      | -192     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 778      |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 81566    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.47e-05 |\n",
      "|    n_updates        | 20141    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.18e-05 |\n",
      "|    n_updates        | 20249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 190      |\n",
      "|    ep_rew_mean      | -190     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 777      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 82210    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.88e-05 |\n",
      "|    n_updates        | 20302    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 189      |\n",
      "|    ep_rew_mean      | -189     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 778      |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 82861    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.22e-05 |\n",
      "|    n_updates        | 20465    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.49e-06 |\n",
      "|    n_updates        | 20499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 187      |\n",
      "|    ep_rew_mean      | -187     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 777      |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 83497    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.87e-05 |\n",
      "|    n_updates        | 20624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.97e-05 |\n",
      "|    n_updates        | 20749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 186      |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 776      |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 84209    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.82e-05 |\n",
      "|    n_updates        | 20802    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 186      |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 777      |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 84966    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.01e-05 |\n",
      "|    n_updates        | 20991    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.65e-05 |\n",
      "|    n_updates        | 20999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 185      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 776      |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 85689    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000104 |\n",
      "|    n_updates        | 21172    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.2e-06  |\n",
      "|    n_updates        | 21249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 184      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 775      |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 86388    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.32e-05 |\n",
      "|    n_updates        | 21346    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.08e-05 |\n",
      "|    n_updates        | 21499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 183      |\n",
      "|    ep_rew_mean      | -183     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 774      |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 87118    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.05e-05 |\n",
      "|    n_updates        | 21529    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 184      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 776      |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 87918    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000172 |\n",
      "|    n_updates        | 21729    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000115 |\n",
      "|    n_updates        | 21749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 185      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 775      |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 88718    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000146 |\n",
      "|    n_updates        | 21929    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-154.80 +/- 55.37\n",
      "Episode length: 154.80 +/- 55.37\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 155      |\n",
      "|    mean_reward      | -155     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.48e-05 |\n",
      "|    n_updates        | 21999    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 185      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 774      |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 89492    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.01e-05 |\n",
      "|    n_updates        | 22122    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000262 |\n",
      "|    n_updates        | 22249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 186      |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 773      |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 90269    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.36e-05 |\n",
      "|    n_updates        | 22317    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.33e-05 |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 186      |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 772      |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 91056    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000375 |\n",
      "|    n_updates        | 22513    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 186      |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 774      |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 91794    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0002   |\n",
      "|    n_updates        | 22698    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.03e-05 |\n",
      "|    n_updates        | 22749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 187      |\n",
      "|    ep_rew_mean      | -187     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 773      |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 92594    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.03e-06 |\n",
      "|    n_updates        | 22898    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.61e-05 |\n",
      "|    n_updates        | 22999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 187      |\n",
      "|    ep_rew_mean      | -187     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 772      |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 93394    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00021  |\n",
      "|    n_updates        | 23098    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.88e-05 |\n",
      "|    n_updates        | 23249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 186      |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 772      |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 94183    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.47e-05 |\n",
      "|    n_updates        | 23295    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 186      |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 774      |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 94924    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.15e-05 |\n",
      "|    n_updates        | 23480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.53e-05 |\n",
      "|    n_updates        | 23499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 185      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 772      |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 95568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.52e-05 |\n",
      "|    n_updates        | 23641    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.17e-05 |\n",
      "|    n_updates        | 23749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 184      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 771      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 96234    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.34e-05 |\n",
      "|    n_updates        | 23808    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.8e-05  |\n",
      "|    n_updates        | 23999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 184      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 770      |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 97017    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.64e-05 |\n",
      "|    n_updates        | 24004    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 184      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 772      |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 97817    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000115 |\n",
      "|    n_updates        | 24204    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000401 |\n",
      "|    n_updates        | 24249    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 184      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 771      |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 98578    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.36e-05 |\n",
      "|    n_updates        | 24394    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.09e-05 |\n",
      "|    n_updates        | 24499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 185      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 771      |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 99378    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.52e-05 |\n",
      "|    n_updates        | 24594    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -200     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000114 |\n",
      "|    n_updates        | 24749    |\n",
      "----------------------------------\n",
      "Trening zakończony w 129.83 sekund\n",
      "\n",
      "Trening modelu dla Acrobot-v1 przy użyciu cuda...\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 500      |\n",
      "|    ep_rew_mean        | -500     |\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | -0.0748  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -2.56    |\n",
      "|    value_loss         | 8.47     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.0384   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -2.07    |\n",
      "|    value_loss         | 6.37     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -500     |\n",
      "| time/              |          |\n",
      "|    fps             | 276      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 500      |\n",
      "|    ep_rew_mean        | -500     |\n",
      "| time/                 |          |\n",
      "|    fps                | 326      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.0539   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -1.93    |\n",
      "|    value_loss         | 5.82     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.987   |\n",
      "|    explained_variance | 0.00281  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -1.61    |\n",
      "|    value_loss         | 5.15     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -500     |\n",
      "| time/              |          |\n",
      "|    fps             | 276      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 500      |\n",
      "|    ep_rew_mean        | -500     |\n",
      "| time/                 |          |\n",
      "|    fps                | 305      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | -0.00078 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -2.05    |\n",
      "|    value_loss         | 4.51     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.08     |\n",
      "|    explained_variance | -0.000795 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -1.94     |\n",
      "|    value_loss         | 3.88      |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -500     |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 500      |\n",
      "|    ep_rew_mean        | -500     |\n",
      "| time/                 |          |\n",
      "|    fps                | 298      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.00154  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -1.92    |\n",
      "|    value_loss         | 3.32     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | -0.00206 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -1.69    |\n",
      "|    value_loss         | 2.73     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -500     |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 800      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 500      |\n",
      "|    ep_rew_mean        | -500     |\n",
      "| time/                 |          |\n",
      "|    fps                | 294      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.000384 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -1.57    |\n",
      "|    value_loss         | 2.33     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.207    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.991   |\n",
      "|    value_loss         | 1.53     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -500     |\n",
      "| time/              |          |\n",
      "|    fps             | 279      |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 500       |\n",
      "|    ep_rew_mean        | -500      |\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.09     |\n",
      "|    explained_variance | -9.92e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -1.22     |\n",
      "|    value_loss         | 1.51      |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-354.60 +/- 178.65\n",
      "Episode length: 355.00 +/- 178.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 355      |\n",
      "|    mean_reward        | -355     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | -0.103   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.755   |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -500     |\n",
      "| time/              |          |\n",
      "|    fps             | 285      |\n",
      "|    iterations      | 1200     |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 463       |\n",
      "|    ep_rew_mean        | -463      |\n",
      "| time/                 |           |\n",
      "|    fps                | 296       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.97     |\n",
      "|    value_loss         | 0.998     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 1.18e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.839   |\n",
      "|    value_loss         | 0.735    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 460      |\n",
      "|    ep_rew_mean     | -460     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 1400     |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 455      |\n",
      "|    ep_rew_mean        | -455     |\n",
      "| time/                 |          |\n",
      "|    fps                | 293      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 4.08e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.661   |\n",
      "|    value_loss         | 0.513    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.766   |\n",
      "|    explained_variance | -0.00199 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.257   |\n",
      "|    value_loss         | 0.34     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 436      |\n",
      "|    ep_rew_mean     | -435     |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 1600     |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 404      |\n",
      "|    ep_rew_mean        | -404     |\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.396   |\n",
      "|    value_loss         | 0.206    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | -0.806   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.251   |\n",
      "|    value_loss         | 0.13     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 391      |\n",
      "|    ep_rew_mean     | -391     |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 1800     |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 375      |\n",
      "|    ep_rew_mean        | -375     |\n",
      "| time/                 |          |\n",
      "|    fps                | 288      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.787   |\n",
      "|    explained_variance | 0.377    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.101    |\n",
      "|    value_loss         | 0.0683   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.894   |\n",
      "|    explained_variance | -0.145   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.053   |\n",
      "|    value_loss         | 0.00971  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 354      |\n",
      "|    ep_rew_mean     | -353     |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 317      |\n",
      "|    ep_rew_mean        | -317     |\n",
      "| time/                 |          |\n",
      "|    fps                | 287      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | -0.00109 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.0207  |\n",
      "|    value_loss         | 0.000197 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.821   |\n",
      "|    explained_variance | -10.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.00376 |\n",
      "|    value_loss         | 6.33e-05 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -297     |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 2200     |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 286       |\n",
      "|    ep_rew_mean        | -285      |\n",
      "| time/                 |           |\n",
      "|    fps                | 286       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.664    |\n",
      "|    explained_variance | 0.0563    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -0.000894 |\n",
      "|    value_loss         | 2.15e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-419.00 +/- 162.00\n",
      "Episode length: 419.20 +/- 161.60\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 419       |\n",
      "|    mean_reward        | -419      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.878    |\n",
      "|    explained_variance | 0.012     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -0.000997 |\n",
      "|    value_loss         | 7.16e-07  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 271      |\n",
      "|    ep_rew_mean     | -270     |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 2400     |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 265      |\n",
      "|    ep_rew_mean        | -264     |\n",
      "| time/                 |          |\n",
      "|    fps                | 287      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.00296  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -0.00376 |\n",
      "|    value_loss         | 2.68e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-85.00 +/- 9.84\n",
      "Episode length: 86.00 +/- 9.84\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 86        |\n",
      "|    mean_reward        | -85       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.806    |\n",
      "|    explained_variance | -0.00774  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -0.000747 |\n",
      "|    value_loss         | 1.76e-06  |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 259      |\n",
      "|    ep_rew_mean     | -258     |\n",
      "| time/              |          |\n",
      "|    fps             | 290      |\n",
      "|    iterations      | 2600     |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 252      |\n",
      "|    ep_rew_mean        | -252     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | -1.93    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.00137 |\n",
      "|    value_loss         | 2.17e-06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.891   |\n",
      "|    explained_variance | 0.0204   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.00118 |\n",
      "|    value_loss         | 4.82e-06 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 249      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    fps             | 290      |\n",
      "|    iterations      | 2800     |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 245      |\n",
      "|    ep_rew_mean        | -244     |\n",
      "| time/                 |          |\n",
      "|    fps                | 294      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.5     |\n",
      "|    explained_variance | 0.00917  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -0.00041 |\n",
      "|    value_loss         | 1.11e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-264.40 +/- 193.28\n",
      "Episode length: 265.00 +/- 192.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 265      |\n",
      "|    mean_reward        | -264     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.966   |\n",
      "|    explained_variance | 0.000456 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 52.4     |\n",
      "|    value_loss         | 5.87e+03 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 238      |\n",
      "|    ep_rew_mean     | -237     |\n",
      "| time/              |          |\n",
      "|    fps             | 293      |\n",
      "|    iterations      | 3000     |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 231      |\n",
      "|    ep_rew_mean        | -230     |\n",
      "| time/                 |          |\n",
      "|    fps                | 297      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.709   |\n",
      "|    explained_variance | 0.082    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.00162 |\n",
      "|    value_loss         | 1.2e-05  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-249.00 +/- 205.03\n",
      "Episode length: 249.60 +/- 204.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 250      |\n",
      "|    mean_reward        | -249     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.536   |\n",
      "|    explained_variance | 0.0269   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.00223 |\n",
      "|    value_loss         | 6.66e-06 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | -221     |\n",
      "| time/              |          |\n",
      "|    fps             | 297      |\n",
      "|    iterations      | 3200     |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 218      |\n",
      "|    ep_rew_mean        | -217     |\n",
      "| time/                 |          |\n",
      "|    fps                | 301      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | -584     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 0.117    |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-415.20 +/- 169.60\n",
      "Episode length: 415.40 +/- 169.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 415      |\n",
      "|    mean_reward        | -415     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.766   |\n",
      "|    explained_variance | 0.0858   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.00171 |\n",
      "|    value_loss         | 3.31e-06 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 214      |\n",
      "|    ep_rew_mean     | -213     |\n",
      "| time/              |          |\n",
      "|    fps             | 297      |\n",
      "|    iterations      | 3400     |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 209      |\n",
      "|    ep_rew_mean        | -209     |\n",
      "| time/                 |          |\n",
      "|    fps                | 301      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.851   |\n",
      "|    explained_variance | -0.786   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -0.00164 |\n",
      "|    value_loss         | 2.06e-06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | 7.69e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.00269 |\n",
      "|    value_loss         | 2.71e-05 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -201     |\n",
      "| time/              |          |\n",
      "|    fps             | 297      |\n",
      "|    iterations      | 3600     |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 196       |\n",
      "|    ep_rew_mean        | -195      |\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.408    |\n",
      "|    explained_variance | 0.00532   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | -0.000497 |\n",
      "|    value_loss         | 7.36e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-344.00 +/- 192.49\n",
      "Episode length: 344.40 +/- 192.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 344       |\n",
      "|    mean_reward        | -344      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.358    |\n",
      "|    explained_variance | 1.53e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -0.000248 |\n",
      "|    value_loss         | 7.57e-06  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 191      |\n",
      "|    ep_rew_mean     | -190     |\n",
      "| time/              |          |\n",
      "|    fps             | 298      |\n",
      "|    iterations      | 3800     |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 170      |\n",
      "|    ep_rew_mean        | -169     |\n",
      "| time/                 |          |\n",
      "|    fps                | 301      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.848   |\n",
      "|    explained_variance | 8.4e-06  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | -0.0028  |\n",
      "|    value_loss         | 3.88e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-249.60 +/- 204.56\n",
      "Episode length: 250.20 +/- 204.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 250      |\n",
      "|    mean_reward        | -250     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.385   |\n",
      "|    explained_variance | -96.3    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -3.5e-05 |\n",
      "|    value_loss         | 5.32e-07 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 153      |\n",
      "|    ep_rew_mean     | -152     |\n",
      "| time/              |          |\n",
      "|    fps             | 301      |\n",
      "|    iterations      | 4000     |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 139      |\n",
      "|    ep_rew_mean        | -138     |\n",
      "| time/                 |          |\n",
      "|    fps                | 304      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 67       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.117   |\n",
      "|    explained_variance | 0.494    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.00175 |\n",
      "|    value_loss         | 5.84e-06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-88.40 +/- 18.27\n",
      "Episode length: 89.40 +/- 18.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 89.4     |\n",
      "|    mean_reward        | -88.4    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.494   |\n",
      "|    explained_variance | 0.00428  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -0.00298 |\n",
      "|    value_loss         | 5.1e-05  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | -127     |\n",
      "| time/              |          |\n",
      "|    fps             | 305      |\n",
      "|    iterations      | 4200     |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 119      |\n",
      "|    ep_rew_mean        | -118     |\n",
      "| time/                 |          |\n",
      "|    fps                | 308      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.235   |\n",
      "|    explained_variance | -0.0151  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.00174 |\n",
      "|    value_loss         | 2.6e-05  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-180.00 +/- 162.01\n",
      "Episode length: 180.80 +/- 161.61\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 181       |\n",
      "|    mean_reward        | -180      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.305    |\n",
      "|    explained_variance | -0.00507  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -0.000599 |\n",
      "|    value_loss         | 2.53e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 116      |\n",
      "|    ep_rew_mean     | -115     |\n",
      "| time/              |          |\n",
      "|    fps             | 308      |\n",
      "|    iterations      | 4400     |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 114      |\n",
      "|    ep_rew_mean        | -113     |\n",
      "| time/                 |          |\n",
      "|    fps                | 311      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.112   |\n",
      "|    explained_variance | 0.403    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 6.74e-05 |\n",
      "|    value_loss         | 9.58e-06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-78.20 +/- 10.85\n",
      "Episode length: 79.20 +/- 10.85\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 79.2      |\n",
      "|    mean_reward        | -78.2     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.269    |\n",
      "|    explained_variance | -0.00562  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | -0.000549 |\n",
      "|    value_loss         | 1.33e-05  |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 112      |\n",
      "|    ep_rew_mean     | -111     |\n",
      "| time/              |          |\n",
      "|    fps             | 313      |\n",
      "|    iterations      | 4600     |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 108      |\n",
      "|    ep_rew_mean        | -107     |\n",
      "| time/                 |          |\n",
      "|    fps                | 315      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.267   |\n",
      "|    explained_variance | -682     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -0.0319  |\n",
      "|    value_loss         | 0.000249 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-83.00 +/- 12.71\n",
      "Episode length: 84.00 +/- 12.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 84       |\n",
      "|    mean_reward        | -83      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.297   |\n",
      "|    explained_variance | 0.00264  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -0.00236 |\n",
      "|    value_loss         | 3.4e-05  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -103     |\n",
      "| time/              |          |\n",
      "|    fps             | 317      |\n",
      "|    iterations      | 4800     |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 100       |\n",
      "|    ep_rew_mean        | -99.1     |\n",
      "| time/                 |           |\n",
      "|    fps                | 319       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.234    |\n",
      "|    explained_variance | 0.0022    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -0.000343 |\n",
      "|    value_loss         | 8.49e-05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-83.60 +/- 9.67\n",
      "Episode length: 84.60 +/- 9.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 84.6     |\n",
      "|    mean_reward        | -83.6    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.566   |\n",
      "|    explained_variance | 2.55e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 12.1     |\n",
      "|    value_loss         | 2e+03    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.5     |\n",
      "|    ep_rew_mean     | -96.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 320      |\n",
      "|    iterations      | 5000     |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 96.9      |\n",
      "|    ep_rew_mean        | -95.9     |\n",
      "| time/                 |           |\n",
      "|    fps                | 323       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.058    |\n",
      "|    explained_variance | -2        |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | -3.58e-05 |\n",
      "|    value_loss         | 8.43e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-93.80 +/- 16.31\n",
      "Episode length: 94.80 +/- 16.31\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 94.8      |\n",
      "|    mean_reward        | -93.8     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.12     |\n",
      "|    explained_variance | -71.2     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -0.000149 |\n",
      "|    value_loss         | 4.85e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.5     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 324      |\n",
      "|    iterations      | 5200     |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 90.8      |\n",
      "|    ep_rew_mean        | -89.8     |\n",
      "| time/                 |           |\n",
      "|    fps                | 326       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.195    |\n",
      "|    explained_variance | -0.00653  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -0.000133 |\n",
      "|    value_loss         | 2.3e-05   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-165.00 +/- 167.92\n",
      "Episode length: 165.80 +/- 167.52\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 166       |\n",
      "|    mean_reward        | -165      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0254   |\n",
      "|    explained_variance | -0.00438  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | -1.61e-05 |\n",
      "|    value_loss         | 2.25e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.8     |\n",
      "|    ep_rew_mean     | -89.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 326      |\n",
      "|    iterations      | 5400     |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 90.2      |\n",
      "|    ep_rew_mean        | -89.2     |\n",
      "| time/                 |           |\n",
      "|    fps                | 329       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.138    |\n",
      "|    explained_variance | -0.00539  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -7.78e-05 |\n",
      "|    value_loss         | 2.25e-05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-199.60 +/- 158.39\n",
      "Episode length: 200.40 +/- 158.01\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 200       |\n",
      "|    mean_reward        | -200      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.178    |\n",
      "|    explained_variance | -0.0923   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -0.000202 |\n",
      "|    value_loss         | 4.42e-06  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.1     |\n",
      "|    ep_rew_mean     | -90.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 328      |\n",
      "|    iterations      | 5600     |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 90.3     |\n",
      "|    ep_rew_mean        | -89.3    |\n",
      "| time/                 |          |\n",
      "|    fps                | 330      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 86       |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.196   |\n",
      "|    explained_variance | 0.508    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -0.00045 |\n",
      "|    value_loss         | 6.59e-07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-79.80 +/- 11.16\n",
      "Episode length: 80.80 +/- 11.16\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 80.8      |\n",
      "|    mean_reward        | -79.8     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.212    |\n",
      "|    explained_variance | -0.0131   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -0.000577 |\n",
      "|    value_loss         | 1.54e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.8     |\n",
      "|    ep_rew_mean     | -87.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 331      |\n",
      "|    iterations      | 5800     |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 87.9      |\n",
      "|    ep_rew_mean        | -86.9     |\n",
      "| time/                 |           |\n",
      "|    fps                | 333       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0135   |\n",
      "|    explained_variance | -9.42     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -2.62e-06 |\n",
      "|    value_loss         | 1.16e-05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-251.40 +/- 203.31\n",
      "Episode length: 252.00 +/- 202.82\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 252       |\n",
      "|    mean_reward        | -251      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.182    |\n",
      "|    explained_variance | -1.16     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -0.000209 |\n",
      "|    value_loss         | 1.11e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.5     |\n",
      "|    ep_rew_mean     | -87.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 332      |\n",
      "|    iterations      | 6000     |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 89.5      |\n",
      "|    ep_rew_mean        | -88.5     |\n",
      "| time/                 |           |\n",
      "|    fps                | 334       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.016    |\n",
      "|    explained_variance | -1.43e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 4.93e-06  |\n",
      "|    value_loss         | 7.53e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-249.60 +/- 204.48\n",
      "Episode length: 250.20 +/- 203.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 250      |\n",
      "|    mean_reward        | -250     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.166   |\n",
      "|    explained_variance | -0.00153 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | -0.00221 |\n",
      "|    value_loss         | 0.000101 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.2     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 333      |\n",
      "|    iterations      | 6200     |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 31000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 95.2     |\n",
      "|    ep_rew_mean        | -94.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 93       |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0222  |\n",
      "|    explained_variance | 0.059    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.325    |\n",
      "|    value_loss         | 9.59e+03 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-331.80 +/- 206.03\n",
      "Episode length: 332.20 +/- 205.54\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 332       |\n",
      "|    mean_reward        | -332      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.116    |\n",
      "|    explained_variance | -0.00774  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | -5.75e-05 |\n",
      "|    value_loss         | 2.24e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.1     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 333      |\n",
      "|    iterations      | 6400     |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 95.7     |\n",
      "|    ep_rew_mean        | -94.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 96       |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0446  |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 5.08e-06 |\n",
      "|    value_loss         | 1.94e-07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-413.00 +/- 174.00\n",
      "Episode length: 413.20 +/- 173.60\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 413       |\n",
      "|    mean_reward        | -413      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00922  |\n",
      "|    explained_variance | -4.67e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | -9.74e-05 |\n",
      "|    value_loss         | 0.0143    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.3     |\n",
      "|    ep_rew_mean     | -95.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 332      |\n",
      "|    iterations      | 6600     |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 33000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 96.2     |\n",
      "|    ep_rew_mean        | -95.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 334      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0677  |\n",
      "|    explained_variance | -555     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.00275  |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0995   |\n",
      "|    explained_variance | -0.596    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -1.39e-05 |\n",
      "|    value_loss         | 2.33e-07  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.5     |\n",
      "|    ep_rew_mean     | -96.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 330      |\n",
      "|    iterations      | 6800     |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 98       |\n",
      "|    ep_rew_mean        | -97      |\n",
      "| time/                 |          |\n",
      "|    fps                | 332      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 103      |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.475   |\n",
      "|    explained_variance | 1.07e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -0.00496 |\n",
      "|    value_loss         | 5.18e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-422.20 +/- 155.60\n",
      "Episode length: 422.40 +/- 155.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 422      |\n",
      "|    mean_reward        | -422     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.144   |\n",
      "|    explained_variance | -22.3    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | -0.00096 |\n",
      "|    value_loss         | 8.45e-05 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.3     |\n",
      "|    ep_rew_mean     | -98.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 329      |\n",
      "|    iterations      | 7000     |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 99.4      |\n",
      "|    ep_rew_mean        | -98.4     |\n",
      "| time/                 |           |\n",
      "|    fps                | 331       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.31     |\n",
      "|    explained_variance | 1.31e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -0.000607 |\n",
      "|    value_loss         | 1.02e-05  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.091    |\n",
      "|    explained_variance | 0.159     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -3.96e-05 |\n",
      "|    value_loss         | 1.69e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -100     |\n",
      "| time/              |          |\n",
      "|    fps             | 328      |\n",
      "|    iterations      | 7200     |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 103       |\n",
      "|    ep_rew_mean        | -102      |\n",
      "| time/                 |           |\n",
      "|    fps                | 330       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0455   |\n",
      "|    explained_variance | 1.71e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -5.74e-06 |\n",
      "|    value_loss         | 7.53e-07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0649  |\n",
      "|    explained_variance | 2.23e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 1.56e-05 |\n",
      "|    value_loss         | 5.26e-06 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -105     |\n",
      "| time/              |          |\n",
      "|    fps             | 327      |\n",
      "|    iterations      | 7400     |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 37000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 111      |\n",
      "|    ep_rew_mean        | -110     |\n",
      "| time/                 |          |\n",
      "|    fps                | 328      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | 9.54e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | -4.8e-05 |\n",
      "|    value_loss         | 1.33e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00676 |\n",
      "|    explained_variance | -53.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 0.000208 |\n",
      "|    value_loss         | 0.18     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 112      |\n",
      "|    ep_rew_mean     | -111     |\n",
      "| time/              |          |\n",
      "|    fps             | 325      |\n",
      "|    iterations      | 7600     |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 116      |\n",
      "|    ep_rew_mean        | -115     |\n",
      "| time/                 |          |\n",
      "|    fps                | 327      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 117      |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.289   |\n",
      "|    explained_variance | 6.56e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -0.00286 |\n",
      "|    value_loss         | 1.97e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0273   |\n",
      "|    explained_variance | -0.0235   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | -1.62e-06 |\n",
      "|    value_loss         | 1.84e-07  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 118      |\n",
      "|    ep_rew_mean     | -117     |\n",
      "| time/              |          |\n",
      "|    fps             | 324      |\n",
      "|    iterations      | 7800     |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 39000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 122      |\n",
      "|    ep_rew_mean        | -121     |\n",
      "| time/                 |          |\n",
      "|    fps                | 325      |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00502 |\n",
      "|    explained_variance | -0.384   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | -4.9e-05 |\n",
      "|    value_loss         | 0.00801  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.021    |\n",
      "|    explained_variance | -0.724    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -7.67e-06 |\n",
      "|    value_loss         | 1.14e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | -123     |\n",
      "| time/              |          |\n",
      "|    fps             | 323      |\n",
      "|    iterations      | 8000     |\n",
      "|    time_elapsed    | 123      |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 125      |\n",
      "|    ep_rew_mean        | -124     |\n",
      "| time/                 |          |\n",
      "|    fps                | 324      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 124      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.0928   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -3.2e-05 |\n",
      "|    value_loss         | 1.46e-07 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.694   |\n",
      "|    explained_variance | 0.00936  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -0.00125 |\n",
      "|    value_loss         | 2.7e-06  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | -126     |\n",
      "| time/              |          |\n",
      "|    fps             | 321      |\n",
      "|    iterations      | 8200     |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 41000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 129       |\n",
      "|    ep_rew_mean        | -128      |\n",
      "| time/                 |           |\n",
      "|    fps                | 323       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0548   |\n",
      "|    explained_variance | 1.82e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | -9.52e-06 |\n",
      "|    value_loss         | 7.32e-07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0916   |\n",
      "|    explained_variance | 1.01e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | -2.26e-05 |\n",
      "|    value_loss         | 1.28e-06  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 133      |\n",
      "|    ep_rew_mean     | -132     |\n",
      "| time/              |          |\n",
      "|    fps             | 320      |\n",
      "|    iterations      | 8400     |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 136       |\n",
      "|    ep_rew_mean        | -135      |\n",
      "| time/                 |           |\n",
      "|    fps                | 322       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0818   |\n",
      "|    explained_variance | -0.0166   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -8.51e-06 |\n",
      "|    value_loss         | 8.63e-07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.208    |\n",
      "|    explained_variance | -1.01e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -0.000124 |\n",
      "|    value_loss         | 5.35e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 139      |\n",
      "|    ep_rew_mean     | -138     |\n",
      "| time/              |          |\n",
      "|    fps             | 319      |\n",
      "|    iterations      | 8600     |\n",
      "|    time_elapsed    | 134      |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 142       |\n",
      "|    ep_rew_mean        | -141      |\n",
      "| time/                 |           |\n",
      "|    fps                | 321       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.239    |\n",
      "|    explained_variance | 9.89e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -0.000105 |\n",
      "|    value_loss         | 5.29e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-342.40 +/- 193.36\n",
      "Episode length: 342.80 +/- 192.87\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 343       |\n",
      "|    mean_reward        | -342      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.213    |\n",
      "|    explained_variance | 5.07e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -0.000205 |\n",
      "|    value_loss         | 4.1e-05   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    fps             | 319      |\n",
      "|    iterations      | 8800     |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 144      |\n",
      "|    ep_rew_mean        | -143     |\n",
      "| time/                 |          |\n",
      "|    fps                | 321      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00154 |\n",
      "|    explained_variance | -12.5    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 5.28e-06 |\n",
      "|    value_loss         | 0.00114  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0429  |\n",
      "|    explained_variance | -4.07    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 0.00021  |\n",
      "|    value_loss         | 0.000403 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -141     |\n",
      "| time/              |          |\n",
      "|    fps             | 318      |\n",
      "|    iterations      | 9000     |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 142      |\n",
      "|    ep_rew_mean        | -141     |\n",
      "| time/                 |          |\n",
      "|    fps                | 320      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 142      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0103  |\n",
      "|    explained_variance | 0.00103  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 0.00978  |\n",
      "|    value_loss         | 5.86e+03 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-417.20 +/- 165.60\n",
      "Episode length: 417.40 +/- 165.20\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 417       |\n",
      "|    mean_reward        | -417      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0202   |\n",
      "|    explained_variance | 0.00265   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -2.22e-05 |\n",
      "|    value_loss         | 1.48e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 145      |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    fps             | 318      |\n",
      "|    iterations      | 9200     |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 146       |\n",
      "|    ep_rew_mean        | -145      |\n",
      "| time/                 |           |\n",
      "|    fps                | 319       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0411   |\n",
      "|    explained_variance | 7.34e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -3.48e-06 |\n",
      "|    value_loss         | 1.78e-07  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0144   |\n",
      "|    explained_variance | 3.36e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | -6.31e-06 |\n",
      "|    value_loss         | 1.4e-05   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | -150     |\n",
      "| time/              |          |\n",
      "|    fps             | 317      |\n",
      "|    iterations      | 9400     |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 153       |\n",
      "|    ep_rew_mean        | -152      |\n",
      "| time/                 |           |\n",
      "|    fps                | 318       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0228   |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | -5.24e-08 |\n",
      "|    value_loss         | 6.4e-10   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.15    |\n",
      "|    explained_variance | -67.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | 6.64e-07 |\n",
      "|    value_loss         | 3.29e-09 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 157      |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    fps             | 316      |\n",
      "|    iterations      | 9600     |\n",
      "|    time_elapsed    | 151      |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 161      |\n",
      "|    ep_rew_mean        | -160     |\n",
      "| time/                 |          |\n",
      "|    fps                | 317      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 152      |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.143   |\n",
      "|    explained_variance | -361     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | 0.00343  |\n",
      "|    value_loss         | 7.93e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0176   |\n",
      "|    explained_variance | 0.00251   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | -1.19e-05 |\n",
      "|    value_loss         | 1.66e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 163      |\n",
      "|    ep_rew_mean     | -162     |\n",
      "| time/              |          |\n",
      "|    fps             | 315      |\n",
      "|    iterations      | 9800     |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 163      |\n",
      "|    ep_rew_mean        | -162     |\n",
      "| time/                 |          |\n",
      "|    fps                | 316      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 156      |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0204  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0207   |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -7.01e-08 |\n",
      "|    value_loss         | 6.4e-10   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -166     |\n",
      "| time/              |          |\n",
      "|    fps             | 314      |\n",
      "|    iterations      | 10000    |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 171       |\n",
      "|    ep_rew_mean        | -170      |\n",
      "| time/                 |           |\n",
      "|    fps                | 316       |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0126   |\n",
      "|    explained_variance | 0.111     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | -9.67e-08 |\n",
      "|    value_loss         | 2.56e-09  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0195  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 175      |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    fps             | 314      |\n",
      "|    iterations      | 10200    |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 179      |\n",
      "|    ep_rew_mean        | -178     |\n",
      "| time/                 |          |\n",
      "|    fps                | 315      |\n",
      "|    iterations         | 10300    |\n",
      "|    time_elapsed       | 163      |\n",
      "|    total_timesteps    | 51500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0261  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0105   |\n",
      "|    explained_variance | -31.3     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10399     |\n",
      "|    policy_loss        | -3.04e-08 |\n",
      "|    value_loss         | 7.56e-09  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 183      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 313      |\n",
      "|    iterations      | 10400    |\n",
      "|    time_elapsed    | 165      |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 187       |\n",
      "|    ep_rew_mean        | -186      |\n",
      "| time/                 |           |\n",
      "|    fps                | 314       |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00596  |\n",
      "|    explained_variance | 6.91e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | -1.63e-06 |\n",
      "|    value_loss         | 1.88e-06  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 53000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00329 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 187      |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    fps             | 312      |\n",
      "|    iterations      | 10600    |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 191      |\n",
      "|    ep_rew_mean        | -190     |\n",
      "| time/                 |          |\n",
      "|    fps                | 313      |\n",
      "|    iterations         | 10700    |\n",
      "|    time_elapsed       | 170      |\n",
      "|    total_timesteps    | 53500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00948 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10699    |\n",
      "|    policy_loss        | 2.98e-11 |\n",
      "|    value_loss         | 2.33e-11 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0305   |\n",
      "|    explained_variance | 0.000328  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -1.09e-06 |\n",
      "|    value_loss         | 4.01e-08  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -195     |\n",
      "| time/              |          |\n",
      "|    fps             | 311      |\n",
      "|    iterations      | 10800    |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 200      |\n",
      "|    ep_rew_mean        | -199     |\n",
      "| time/                 |          |\n",
      "|    fps                | 313      |\n",
      "|    iterations         | 10900    |\n",
      "|    time_elapsed       | 174      |\n",
      "|    total_timesteps    | 54500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0714  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10899    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 55000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.02    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10999    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 204      |\n",
      "|    ep_rew_mean     | -203     |\n",
      "| time/              |          |\n",
      "|    fps             | 311      |\n",
      "|    iterations      | 11000    |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 208      |\n",
      "|    ep_rew_mean        | -207     |\n",
      "| time/                 |          |\n",
      "|    fps                | 312      |\n",
      "|    iterations         | 11100    |\n",
      "|    time_elapsed       | 177      |\n",
      "|    total_timesteps    | 55500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0246  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11099    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00585 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 210      |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    fps             | 310      |\n",
      "|    iterations      | 11200    |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | -214     |\n",
      "| time/                 |          |\n",
      "|    fps                | 311      |\n",
      "|    iterations         | 11300    |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 56500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0018  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | 8.18e-08 |\n",
      "|    value_loss         | 2.54e-06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 57000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00808 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11399    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 216      |\n",
      "|    ep_rew_mean     | -215     |\n",
      "| time/              |          |\n",
      "|    fps             | 310      |\n",
      "|    iterations      | 11400    |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 220      |\n",
      "|    ep_rew_mean        | -219     |\n",
      "| time/                 |          |\n",
      "|    fps                | 311      |\n",
      "|    iterations         | 11500    |\n",
      "|    time_elapsed       | 184      |\n",
      "|    total_timesteps    | 57500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0488  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11499    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00836 |\n",
      "|    explained_variance | -0.72    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | 2.02e-08 |\n",
      "|    value_loss         | 2.68e-10 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 227      |\n",
      "|    ep_rew_mean     | -226     |\n",
      "| time/              |          |\n",
      "|    fps             | 309      |\n",
      "|    iterations      | 11600    |\n",
      "|    time_elapsed    | 187      |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 229       |\n",
      "|    ep_rew_mean        | -228      |\n",
      "| time/                 |           |\n",
      "|    fps                | 310       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 188       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00177  |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -2.48e-09 |\n",
      "|    value_loss         | 6.75e-10  |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 59000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0149  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 233      |\n",
      "|    ep_rew_mean     | -232     |\n",
      "| time/              |          |\n",
      "|    fps             | 309      |\n",
      "|    iterations      | 11800    |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 236       |\n",
      "|    ep_rew_mean        | -236      |\n",
      "| time/                 |           |\n",
      "|    fps                | 310       |\n",
      "|    iterations         | 11900     |\n",
      "|    time_elapsed       | 191       |\n",
      "|    total_timesteps    | 59500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0233   |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11899     |\n",
      "|    policy_loss        | -2.12e-08 |\n",
      "|    value_loss         | 0         |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00527 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11999    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 241      |\n",
      "|    ep_rew_mean     | -240     |\n",
      "| time/              |          |\n",
      "|    fps             | 308      |\n",
      "|    iterations      | 12000    |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 244      |\n",
      "|    ep_rew_mean        | -243     |\n",
      "| time/                 |          |\n",
      "|    fps                | 309      |\n",
      "|    iterations         | 12100    |\n",
      "|    time_elapsed       | 195      |\n",
      "|    total_timesteps    | 60500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0858  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 61000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0226  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 248      |\n",
      "|    ep_rew_mean     | -247     |\n",
      "| time/              |          |\n",
      "|    fps             | 308      |\n",
      "|    iterations      | 12200    |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 252      |\n",
      "|    ep_rew_mean        | -251     |\n",
      "| time/                 |          |\n",
      "|    fps                | 309      |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 198      |\n",
      "|    total_timesteps    | 61500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0468  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00912 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 255      |\n",
      "|    ep_rew_mean     | -254     |\n",
      "| time/              |          |\n",
      "|    fps             | 307      |\n",
      "|    iterations      | 12400    |\n",
      "|    time_elapsed    | 201      |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 259      |\n",
      "|    ep_rew_mean        | -258     |\n",
      "| time/                 |          |\n",
      "|    fps                | 308      |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 202      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00369 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 63000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0108  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12599    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 261      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    fps             | 307      |\n",
      "|    iterations      | 12600    |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 264      |\n",
      "|    ep_rew_mean        | -264     |\n",
      "| time/                 |          |\n",
      "|    fps                | 308      |\n",
      "|    iterations         | 12700    |\n",
      "|    time_elapsed       | 206      |\n",
      "|    total_timesteps    | 63500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00669 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12699    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.015   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12799    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 264      |\n",
      "|    ep_rew_mean     | -264     |\n",
      "| time/              |          |\n",
      "|    fps             | 306      |\n",
      "|    iterations      | 12800    |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 268      |\n",
      "|    ep_rew_mean        | -267     |\n",
      "| time/                 |          |\n",
      "|    fps                | 307      |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 209      |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00823 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 65000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00716 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12999    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 271      |\n",
      "|    ep_rew_mean     | -270     |\n",
      "| time/              |          |\n",
      "|    fps             | 306      |\n",
      "|    iterations      | 13000    |\n",
      "|    time_elapsed    | 212      |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 272      |\n",
      "|    ep_rew_mean        | -272     |\n",
      "| time/                 |          |\n",
      "|    fps                | 307      |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 213      |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00687 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0283  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 276      |\n",
      "|    ep_rew_mean     | -275     |\n",
      "| time/              |          |\n",
      "|    fps             | 305      |\n",
      "|    iterations      | 13200    |\n",
      "|    time_elapsed    | 215      |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 278       |\n",
      "|    ep_rew_mean        | -277      |\n",
      "| time/                 |           |\n",
      "|    fps                | 306       |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 216       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000947 |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 0         |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | -500     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 67000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00125 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 281      |\n",
      "|    ep_rew_mean     | -281     |\n",
      "| time/              |          |\n",
      "|    fps             | 305      |\n",
      "|    iterations      | 13400    |\n",
      "|    time_elapsed    | 219      |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 285      |\n",
      "|    ep_rew_mean        | -284     |\n",
      "| time/                 |          |\n",
      "|    fps                | 306      |\n",
      "|    iterations         | 13500    |\n",
      "|    time_elapsed       | 220      |\n",
      "|    total_timesteps    | 67500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00215 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13499    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 68000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000999 |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13599     |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 0         |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 288      |\n",
      "|    ep_rew_mean     | -287     |\n",
      "| time/              |          |\n",
      "|    fps             | 304      |\n",
      "|    iterations      | 13600    |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 288      |\n",
      "|    ep_rew_mean        | -288     |\n",
      "| time/                 |          |\n",
      "|    fps                | 305      |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 223      |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00193 |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.069    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -2.98e-07 |\n",
      "|    value_loss         | 6.4e-10   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 291      |\n",
      "|    ep_rew_mean     | -290     |\n",
      "| time/              |          |\n",
      "|    fps             | 304      |\n",
      "|    iterations      | 13800    |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 295      |\n",
      "|    ep_rew_mean        | -294     |\n",
      "| time/                 |          |\n",
      "|    fps                | 305      |\n",
      "|    iterations         | 13900    |\n",
      "|    time_elapsed       | 227      |\n",
      "|    total_timesteps    | 69500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | -0       |\n",
      "|    value_loss         | 0        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 500       |\n",
      "|    mean_reward        | -500      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 70000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000585 |\n",
      "|    explained_variance | -2.56e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13999     |\n",
      "|    policy_loss        | -1.13e-07 |\n",
      "|    value_loss         | 5.64e-06  |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 299      |\n",
      "|    ep_rew_mean     | -298     |\n",
      "| time/              |          |\n",
      "|    fps             | 304      |\n",
      "|    iterations      | 14000    |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "Trening zakończony w 230.19 sekund\n",
      "\n",
      "Wszystkie modele zostały wytrenowane i zapisane.\n",
      "\n",
      "Podsumowanie wydajności treningu:\n",
      "+----------------+------------+---------------------+---------------------------+\n",
      "| Środowisko     | Algorytm   |   Czas treningu (s) |   Całkowita liczba kroków |\n",
      "+================+============+=====================+===========================+\n",
      "| CartPole-v1    | PPO        |             156.311 |                     50000 |\n",
      "+----------------+------------+---------------------+---------------------------+\n",
      "| MountainCar-v0 | DQN        |             129.835 |                    100000 |\n",
      "+----------------+------------+---------------------+---------------------------+\n",
      "| Acrobot-v1     | A2C        |             230.192 |                     70000 |\n",
      "+----------------+------------+---------------------+---------------------------+\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zadanie 4\n",
    "#### Wykonać test i zebrać wyniki działania modelu (uzyskana nagroda)"
   ],
   "id": "cf50142585de3895"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:36:36.739610Z",
     "start_time": "2025-05-21T13:36:32.728375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, env_name, episodes=10):\n",
    "    env = gym.make(env_name, render_mode=None)\n",
    "    rewards = []\n",
    "    steps = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        terminated = False\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not (done or terminated):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = truncated\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        steps.append(step_count)\n",
    "        print(f\"Epizod {episode+1}: Nagroda = {total_reward:.2f}, Kroki = {step_count}\")\n",
    "\n",
    "    return rewards, steps\n",
    "\n",
    "try:\n",
    "    # Próba wczytania najlepszych modeli, jeśli zostały zapisane\n",
    "    print(\"Wczytywanie wytrenowanych modeli...\")\n",
    "    model1_loaded = PPO.load(\"./trained_models/model1/best_model.zip\", device=device)\n",
    "    model2_loaded = DQN.load(\"./trained_models/model2/best_model.zip\", device=device)\n",
    "    model3_loaded = A2C.load(\"./trained_models/model3/best_model.zip\", device=device)\n",
    "    print(\"Wczytano zapisane modele.\")\n",
    "except FileNotFoundError:\n",
    "    # Jeśli nie ma najlepszych modeli, używamy ostatnio wytrenowanych\n",
    "    try:\n",
    "        print(\"Próba wczytania finalnych modeli...\")\n",
    "        model1_loaded = PPO.load(\"./trained_models/final_ppo_CartPole-v1\", device=device)\n",
    "        model2_loaded = DQN.load(\"./trained_models/final_dqn_MountainCar-v0\", device=device)\n",
    "        model3_loaded = A2C.load(\"./trained_models/final_a2c_Acrobot-v1\", device=device)\n",
    "        print(\"Wczytano finalne modele.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Nie znaleziono zapisanych modeli. Używanie ostatnio wytrenowanych...\")\n",
    "        model1_loaded = model1\n",
    "        model2_loaded = model2\n",
    "        model3_loaded = model3\n",
    "\n",
    "# Ewaluacja każdego modelu\n",
    "print(f\"\\nEwaluacja {env1_name} z PPO:\")\n",
    "rewards1, steps1 = evaluate_model(model1_loaded, env1_name)\n",
    "\n",
    "print(f\"\\nEwaluacja {env2_name} z DQN:\")\n",
    "rewards2, steps2 = evaluate_model(model2_loaded, env2_name)\n",
    "\n",
    "print(f\"\\nEwaluacja {env3_name} z A2C:\")\n",
    "rewards3, steps3 = evaluate_model(model3_loaded, env3_name)\n",
    "\n",
    "# Wizualizacja wyników\n",
    "environments = [env1_name, env2_name, env3_name]\n",
    "avg_rewards = [np.mean(rewards1), np.mean(rewards2), np.mean(rewards3)]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Wykres nagród\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(environments, avg_rewards, color=['blue', 'green', 'red'])\n",
    "plt.title('Średnia nagroda według środowiska')\n",
    "plt.ylabel('Średnia nagroda')\n",
    "for i, v in enumerate(avg_rewards):\n",
    "    plt.text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "# Wykres kroków\n",
    "plt.subplot(2, 2, 2)\n",
    "avg_steps = [np.mean(steps1), np.mean(steps2), np.mean(steps3)]\n",
    "plt.bar(environments, avg_steps, color=['blue', 'green', 'red'])\n",
    "plt.title('Średnia liczba kroków według środowiska')\n",
    "plt.ylabel('Średnia liczba kroków')\n",
    "for i, v in enumerate(avg_steps):\n",
    "    plt.text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "# Wariancja nagrody\n",
    "plt.subplot(2, 2, 3)\n",
    "std_rewards = [np.std(rewards1), np.std(rewards2), np.std(rewards3)]\n",
    "plt.bar(environments, std_rewards, color=['blue', 'green', 'red'])\n",
    "plt.title('Odchylenie standardowe nagrody według środowiska')\n",
    "plt.ylabel('Odchylenie standardowe')\n",
    "for i, v in enumerate(std_rewards):\n",
    "    plt.text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "# Czas treningu na krok\n",
    "plt.subplot(2, 2, 4)\n",
    "timesteps = [timesteps1, timesteps2, timesteps3]\n",
    "training_times = [training_time1, training_time2, training_time3]\n",
    "time_per_step = [t/s for t, s in zip(training_times, timesteps)]\n",
    "plt.bar(environments, time_per_step, color=['blue', 'green', 'red'])\n",
    "plt.title('Czas treningu na krok (ms)')\n",
    "plt.ylabel('Czas (ms)')\n",
    "for i, v in enumerate(time_per_step):\n",
    "    plt.text(i, v + 0.0001, f\"{v*1000:.2f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tworzenie szczegółowej tabeli wyników\n",
    "results_data = {\n",
    "    'Środowisko': environments,\n",
    "    'Algorytm': ['PPO', 'DQN', 'A2C'],\n",
    "    'Średnia nagroda': [f\"{np.mean(rewards1):.2f}\", f\"{np.mean(rewards2):.2f}\", f\"{np.mean(rewards3):.2f}\"],\n",
    "    'Min nagroda': [f\"{np.min(rewards1):.2f}\", f\"{np.min(rewards2):.2f}\", f\"{np.min(rewards3):.2f}\"],\n",
    "    'Max nagroda': [f\"{np.max(rewards1):.2f}\", f\"{np.max(rewards2):.2f}\", f\"{np.max(rewards3):.2f}\"],\n",
    "    'Odch. stand. nagrody': [f\"{np.std(rewards1):.2f}\", f\"{np.std(rewards2):.2f}\", f\"{np.std(rewards3):.2f}\"],\n",
    "    'Średnia liczba kroków': [f\"{np.mean(steps1):.2f}\", f\"{np.mean(steps2):.2f}\", f\"{np.mean(steps3):.2f}\"],\n",
    "    'Czas treningu (s)': [f\"{training_time1:.2f}\", f\"{training_time2:.2f}\", f\"{training_time3:.2f}\"],\n",
    "    'Kroki/sekunda': [f\"{timesteps1/training_time1:.2f}\", f\"{timesteps2/training_time2:.2f}\", f\"{timesteps3/training_time3:.2f}\"]\n",
    "}\n",
    "\n",
    "# Wyświetlanie tabeli wyników\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nWyniki ewaluacji modeli:\")\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid', showindex=False))"
   ],
   "id": "8a75a2ce5a7d8f9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie wytrenowanych modeli...\n",
      "Wczytano zapisane modele.\n",
      "\n",
      "Ewaluacja CartPole-v1 z PPO:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcin/PycharmProjects/arisc/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "/home/marcin/PycharmProjects/arisc/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epizod 1: Nagroda = 374.00, Kroki = 374\n",
      "Epizod 2: Nagroda = 377.00, Kroki = 377\n",
      "Epizod 3: Nagroda = 410.00, Kroki = 410\n",
      "Epizod 4: Nagroda = 500.00, Kroki = 500\n",
      "Epizod 5: Nagroda = 417.00, Kroki = 417\n",
      "Epizod 6: Nagroda = 374.00, Kroki = 374\n",
      "Epizod 7: Nagroda = 500.00, Kroki = 500\n",
      "Epizod 8: Nagroda = 500.00, Kroki = 500\n",
      "Epizod 9: Nagroda = 378.00, Kroki = 378\n",
      "Epizod 10: Nagroda = 500.00, Kroki = 500\n",
      "\n",
      "Ewaluacja MountainCar-v0 z DQN:\n",
      "Epizod 1: Nagroda = -100.00, Kroki = 100\n",
      "Epizod 2: Nagroda = -167.00, Kroki = 167\n",
      "Epizod 3: Nagroda = -97.00, Kroki = 97\n",
      "Epizod 4: Nagroda = -85.00, Kroki = 85\n",
      "Epizod 5: Nagroda = -200.00, Kroki = 200\n",
      "Epizod 6: Nagroda = -200.00, Kroki = 200\n",
      "Epizod 7: Nagroda = -200.00, Kroki = 200\n",
      "Epizod 8: Nagroda = -87.00, Kroki = 87\n",
      "Epizod 9: Nagroda = -94.00, Kroki = 94\n",
      "Epizod 10: Nagroda = -200.00, Kroki = 200\n",
      "\n",
      "Ewaluacja Acrobot-v1 z A2C:\n",
      "Epizod 1: Nagroda = -100.00, Kroki = 101\n",
      "Epizod 2: Nagroda = -71.00, Kroki = 72\n",
      "Epizod 3: Nagroda = -70.00, Kroki = 71\n",
      "Epizod 4: Nagroda = -129.00, Kroki = 130\n",
      "Epizod 5: Nagroda = -82.00, Kroki = 83\n",
      "Epizod 6: Nagroda = -65.00, Kroki = 66\n",
      "Epizod 7: Nagroda = -72.00, Kroki = 73\n",
      "Epizod 8: Nagroda = -500.00, Kroki = 500\n",
      "Epizod 9: Nagroda = -73.00, Kroki = 74\n",
      "Epizod 10: Nagroda = -85.00, Kroki = 86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wyniki ewaluacji modeli:\n",
      "+----------------+------------+-------------------+---------------+---------------+------------------------+-------------------------+---------------------+-----------------+\n",
      "| Środowisko     | Algorytm   |   Średnia nagroda |   Min nagroda |   Max nagroda |   Odch. stand. nagrody |   Średnia liczba kroków |   Czas treningu (s) |   Kroki/sekunda |\n",
      "+================+============+===================+===============+===============+========================+=========================+=====================+=================+\n",
      "| CartPole-v1    | PPO        |             433   |           374 |           500 |                  56.45 |                   433   |              156.31 |          319.88 |\n",
      "+----------------+------------+-------------------+---------------+---------------+------------------------+-------------------------+---------------------+-----------------+\n",
      "| MountainCar-v0 | DQN        |            -143   |          -200 |           -85 |                  51.42 |                   143   |              129.83 |          770.21 |\n",
      "+----------------+------------+-------------------+---------------+---------------+------------------------+-------------------------+---------------------+-----------------+\n",
      "| Acrobot-v1     | A2C        |            -124.7 |          -500 |           -65 |                 126.4  |                   125.6 |              230.19 |          304.09 |\n",
      "+----------------+------------+-------------------+---------------+---------------+------------------------+-------------------------+---------------------+-----------------+\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Wnioski\n",
    "Akceleracja GPU znacząco przyspieszyła trening algorytmów uczenia ze wzmocnieniem, szczególnie DQN w MountainCar, gdzie operacje na buforze doświadczeń były najbardziej złożone obliczeniowo. Algorytmy wykazały różną skuteczność w zależności od środowiska: PPO najlepiej radził sobie w CartPole osiągając wysokie, stabilne wyniki, DQN efektywnie rozwiązywał problem opóźnionej nagrody w MountainCar, a A2C oferował dobry kompromis wydajności w Acrobot. Złożoność środowiska bezpośrednio korelowała z wymaganym czasem treningu - CartPole potrzebował najmniej czasu ze względu na prostą dynamikę, podczas gdy MountainCar i Acrobot wymagały znacznie dłuższego treningu."
   ],
   "id": "7b50c267a8eda054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c9c57edf93ece11b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
